{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнение LSTM и Transformer моделей для автодополнения текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import evaluate\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src import data_utils, split_dataset, next_token_dataset, lstm_model, transformer\n",
    "\n",
    "\n",
    "TOKENIZER = next_token_dataset.TOKENIZER\n",
    "MIN_LEN = next_token_dataset.MIN_LEN\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Используемое устройство: {DEVICE}\")\n",
    "print(f\"Размер словаря: {len(TOKENIZER)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "\n",
    "if not (os.path.exists(\"data/train.txt\") and os.path.exists(\"data/val.txt\") and os.path.exists(\"data/test.txt\")):\n",
    "    print(\"Файлы данных не найдены, загружаем и обрабатываем исходные данные...\")\n",
    "\n",
    "    \n",
    "    url = 'https://code.s3.yandex.net/deep-learning/tweets.txt'\n",
    "    filename = \"data/tweets.txt\"\n",
    "    if not os.path.exists(filename):\n",
    "        os.makedirs(\"data\", exist_ok=True)\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(f\"Загружен файл: {filename}\")\n",
    "    \n",
    "    print(\"Очистка текста...\")\n",
    "    with open(filename, 'r', encoding='utf-8') as infile, \\\n",
    "         open(\"data/cleaned_text.txt\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line in infile:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            cleaned_line = data_utils.clean_text(line)\n",
    "            if cleaned_line:\n",
    "                outfile.write(cleaned_line + '\\n')\n",
    "    print(\"Очистка завершена.\")\n",
    "    \n",
    "    print(\"Разделение на выборки...\")\n",
    "    with open(\"data/cleaned_text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    \n",
    "    splits = split_dataset.split_dataset(lines)\n",
    "    train_texts = splits[\"train\"]\n",
    "    val_texts = splits[\"val\"] \n",
    "    test_texts = splits[\"test\"]\n",
    "    \n",
    "else:\n",
    "    print(\"Файлы данных найдены, загружаем готовые выборки...\")\n",
    "    train_texts = read_texts_from_file(\"data/train.txt\")\n",
    "    val_texts = read_texts_from_file(\"data/val.txt\")\n",
    "    test_texts = read_texts_from_file(\"data/test.txt\")\n",
    "\n",
    "print(f\"Загружено: train({len(train_texts)}), val({len(val_texts)}), test({len(test_texts)})\")\n",
    "\n",
    "train_dataset = next_token_dataset.TextDataset(train_texts, num_targets=1)\n",
    "val_dataset = next_token_dataset.TextDataset(val_texts, num_targets=1)\n",
    "test_dataset = next_token_dataset.TextDataset(test_texts, num_targets=1)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=next_token_dataset.collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=next_token_dataset.collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=next_token_dataset.collate)\n",
    "\n",
    "print(f\"Данные загружены: train({len(train_dataset)}), val({len(val_dataset)}), test({len(test_dataset)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение LSTM модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(TOKENIZER)\n",
    "lstm_model_instance = lstm_model.LstmModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2\n",
    ")\n",
    "\n",
    "print(\"Модель LSTM создана успешно\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    contexts = batch['contexts']\n",
    "    lengths = batch['lengths']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = lstm_model_instance(contexts, lengths)\n",
    "    \n",
    "    print(f\"Входной тензор: {contexts.shape}\")\n",
    "    print(f\"Выходной тензор (логиты): {logits.shape}\")\n",
    "    print(f\"Размер словаря: {logits.size(-1)}\")\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load('rouge')\n",
    "print(\"ROUGE метрика загружена\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Начало обучения LSTM модели...\")\n",
    "train_losses, val_losses, val_accuracies, val_rouge_scores = lstm_model_instance.train_model(\n",
    "    n_epochs=3,\n",
    "    learning_rate=0.001,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    rouge_metric=rouge_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rouge2 равны нулю поскольку мы учимся предсказывать только один токен, а rouge2 используется для биграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Функция потерь')\n",
    "plt.xlabel('Эпоха')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(val_accuracies)\n",
    "plt.title('Точность')\n",
    "plt.xlabel('Эпоха')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "rouge_scores_plot = [score['rouge1'] if score else 0 for score in val_rouge_scores]\n",
    "plt.plot(rouge_scores_plot)\n",
    "plt.title('ROUGE-1')\n",
    "plt.xlabel('Эпоха')\n",
    "plt.ylabel('Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "lstm_model_instance.save_model(\"models/lstm_model.pth\")\n",
    "print(\"Модель сохранена в models/lstm_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Тестирование LSTM модели...\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "test_loss, test_accuracy, test_rouge = lstm_model_instance.evaluate_model(test_loader, criterion, rouge_metric)\n",
    "print(f\"LSTM Test Loss: {test_loss:.4f}\")\n",
    "print(f\"LSTM Test Accuracy: {test_accuracy:.4f}\")\n",
    "if test_rouge:\n",
    "    print(\"LSTM Test ROUGE:\")\n",
    "    for metric, score in test_rouge.items():\n",
    "        print(f\"  {metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задании нужно предсказывать 1/4 часть текста, но поскольку предложения имеют разную длину, а в батче разное количество слов для предсказания, то будем использовать фиксированное число слов например 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_multiword_prediction_data(texts, num_words=3):\n",
    "    \"\"\"Подготовка данных для предсказания нескольких слов\"\"\"\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    full_texts = []\n",
    "    \n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        if len(words) < MIN_LEN + num_words:\n",
    "            continue\n",
    "        ctx_len = len(words) - num_words\n",
    "        context = ' '.join(words[:ctx_len])\n",
    "        target = ' '.join(words[ctx_len:])\n",
    "        contexts.append(context)\n",
    "        targets.append(target)\n",
    "        full_texts.append(context + \" \" + target)\n",
    "    \n",
    "    return contexts, targets, full_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Подготовка данных для предсказания 3 слов...\")\n",
    "lstm_contexts, lstm_targets, full_texts = prepare_multiword_prediction_data(test_texts, NUM_WORDS)\n",
    "print(f\"Подготовлено {len(lstm_contexts)} примеров\")\n",
    "\n",
    "print(\"Предсказание LSTM с авторегрессией...\")\n",
    "lstm_predictions = [\"\"] * len(lstm_contexts)\n",
    "current_contexts = lstm_contexts.copy()\n",
    "\n",
    "for word_idx in range(NUM_WORDS):\n",
    "    print(f\"Генерация слова {word_idx + 1}...\")\n",
    "    batch_predictions = []\n",
    "    \n",
    "    for i in range(0, len(current_contexts), 32):\n",
    "        batch_contexts = current_contexts[i:i+32]\n",
    "        \n",
    "        for context in batch_contexts:\n",
    "            try:\n",
    "                context_ids = torch.tensor(TOKENIZER.encode(context, add_special_tokens=False))\n",
    "                generated_tokens = lstm_model_instance.generate_tokens(\n",
    "                    context_ids, max_length=1, temperature=0.8\n",
    "                )\n",
    "                pred_word = TOKENIZER.decode(generated_tokens[:1], skip_special_tokens=True)\n",
    "                batch_predictions.append(pred_word if pred_word else \"word\")\n",
    "            except:\n",
    "                batch_predictions.append(\"word\")\n",
    "    \n",
    "    current_contexts = [ctx + \" \" + pred for ctx, pred in zip(current_contexts, batch_predictions)]\n",
    "    lstm_predictions = [prev + \" \" + pred for prev, pred in zip(lstm_predictions, batch_predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    lstm_rouge = rouge_metric.compute(\n",
    "        predictions=lstm_predictions,\n",
    "        references=lstm_targets,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    print(\"\\nLSTM ROUGE метрики (предсказание 4 слов):\")\n",
    "    for metric, score in lstm_rouge.items():\n",
    "        print(f\"  {metric}: {score:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка расчета ROUGE для LSTM: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование предобученного трансформера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Загрузка предобученного трансформера distilgpt2...\")\n",
    "transformer_model_instance = transformer.TransformerGenerator('distilgpt2')\n",
    "print(\"Трансформер загружен успешно\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nПредсказание трансформера с авторегрессией...\")\n",
    "transformer_predictions = [\"\"] * len(lstm_contexts)\n",
    "current_contexts_transformer = lstm_contexts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_idx in range(NUM_WORDS):\n",
    "    print(f\"Генерация слова {word_idx + 1} трансформером...\")\n",
    "    batch_preds = transformer_model_instance.generate(\n",
    "        current_contexts_transformer, \n",
    "        max_new_tokens=1\n",
    "    )\n",
    "    \n",
    "    processed_preds = []\n",
    "    for i, (pred, context) in enumerate(zip(batch_preds, current_contexts_transformer)):\n",
    "        if pred.startswith(context):\n",
    "            new_part = pred[len(context):].strip()\n",
    "            first_word = new_part.split()[0] if new_part.split() else \"word\"\n",
    "            processed_preds.append(first_word)\n",
    "        else:\n",
    "            first_word = pred.split()[0] if pred.split() else \"word\"\n",
    "            processed_preds.append(first_word)\n",
    "    \n",
    "    current_contexts_transformer = [ctx + \" \" + pred for ctx, pred in zip(current_contexts_transformer, processed_preds)]\n",
    "    transformer_predictions = [prev + \" \" + pred for prev, pred in zip(transformer_predictions, processed_preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    transformer_rouge = rouge_metric.compute(\n",
    "        predictions=transformer_predictions,\n",
    "        references=lstm_targets,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    print(\"\\nТрансформер ROUGE метрики (предсказание 4 слов):\")\n",
    "    for metric, score in transformer_rouge.items():\n",
    "        print(f\"  {metric}: {score:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка расчета ROUGE для трансформера: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формулирование выводов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Финальное сравнение\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ФИНАЛЬНОЕ СРАВНЕНИЕ (предсказание 4 слов)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nLSTM модель:\")\n",
    "if 'lstm_rouge' in locals():\n",
    "    for metric, score in lstm_rouge.items():\n",
    "        print(f\"  {metric}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nТрансформер (distilgpt2):\")\n",
    "if 'transformer_rouge' in locals():\n",
    "    for metric, score in transformer_rouge.items():\n",
    "        print(f\"  {metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примеры предсказаний\n",
    "print(\"\\nПРИМЕРЫ ПРЕДСКАЗАНИЙ:\")\n",
    "print(\"=\"*40)\n",
    "for i in range(min(3, len(lstm_contexts))):\n",
    "    print(f\"\\nПример {i+1}:\")\n",
    "    print(f\"Контекст: {lstm_contexts[i]}\")\n",
    "    print(f\"Референс: {lstm_targets[i]}\")\n",
    "    if i < len(lstm_predictions):\n",
    "        print(f\"LSTM:     {lstm_predictions[i].strip()}\")\n",
    "    if i < len(transformer_predictions):\n",
    "        print(f\"Трансформер: {transformer_predictions[i].strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Предсказания трансформера более осмысленные, а LSTM выдает бессмысленные словосочентания.\n",
    "- Трансформер лучше улавливает смысл и пытается логически его продолжить, теряет смысл и генерирует случайные слова (фразы).\n",
    "- По численным метрикам хоть и низким трансформер всеже показывается себя лучше."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
